{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj4skvzYh1fV"
   },
   "source": [
    "# Lab 3: Word Embeddings and Language Modelling\n",
    "\n",
    "Adam Ek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDaBH_wIh1fd"
   },
   "source": [
    "In this lab we'll explore constructing *static* word embeddings (i.e. word2vec) and building language models. We'll also evaluate these systems on intermediate tasks, namely word similarity and identifying \"good\" and \"bad\" sentences.\n",
    "\n",
    "* For this we'll use pytorch. Some basic operations that will be useful can be found here: https://jhui.github.io/2018/02/09/PyTorch-Basic-operations\n",
    "* In general: we are not interested in getting state-of-the-art performance :) focus on the implementation and not results of your model. For this reason, you can use a subset of the dataset: the first 5000-10 000 sentences or so, on linux/mac: ```head -n 10000 inputfile > outputfile```. \n",
    "* If possible, use the MLTGpu, it will make everything faster :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "MF8GZSgYh1ff"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# for gpu, replace \"cpu\" with \"cuda:n\" where n is the index of the GPU\n",
    "#device = torch.device('cpu')\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKcUjq-Xh1ff"
   },
   "source": [
    "# Word2Vec embeddings\n",
    "\n",
    "In this first part we'll construct a word2vec model which will give us *static* word embeddings (that is, they are fixed after training).\n",
    "\n",
    "After we've trained our model we will evaluate the embeddings obtained on a word similarity task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf4SBE1rh1fg"
   },
   "source": [
    "## Formatting data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCGz_BNeh1fg"
   },
   "source": [
    "First we need to load some data, you can download the file on canvas under files/03-lab-data/wiki-corpus.txt. The file contains 50 000 sentences randomly selected from the complete wikipedia. Each line in the file contains one sentence. The sentences are whitespace tokenized.\n",
    "\n",
    "Your first task is to create a dataset suitable for word2vec. That is, we define some ```window_size``` then iterate over all sentences in the dataset, putting the center word in one field and the context words in another (separate the fields with ```tab```).\n",
    "\n",
    "For example, the sentece \"this is a lab\" with ```window size = 4``` will be formatted as:\n",
    "```\n",
    "center, context\n",
    "---------------------\n",
    "this    is a\n",
    "is      this a lab\n",
    "a       this is lab\n",
    "lab     is a\n",
    "```\n",
    "\n",
    "this will be our training examples when training the word2vec model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3vza9rNh1fg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import itertools\n",
    "import csv\n",
    "import string\n",
    "\n",
    "\n",
    "data_path = './wiki-corpus.txt'\n",
    "WINDOW_SIZE = 4\n",
    "semi = WINDOW_SIZE//2\n",
    "punct = list(string.punctuation)\n",
    "punct.append('``')\n",
    "\n",
    "def corpus_reader(data_path):\n",
    "    with open(data_path) as f:\n",
    "        content = [line.rstrip() for line in f]\n",
    "        content = content[:10000] # use 10.000 sentences\n",
    "    for index,sentence in enumerate(content):\n",
    "        content[index] = [i for i in word_tokenize(sentence) if i not in punct]\n",
    "\n",
    "    \n",
    "    context = [\"context\"]\n",
    "    center = [\"center\"]\n",
    "    \n",
    "    for sentence in content:\n",
    "        for i,word in enumerate(sentence):\n",
    "            window = []\n",
    "            if i - semi < 0 and i == 1:\n",
    "                window.append(sentence[i-1])\n",
    "            if i - semi >= 0:\n",
    "                window.extend(sentence[i-semi:i])\n",
    "            if i + semi < len(sentence):\n",
    "                window.extend(sentence[(i+1):(i+1+semi)])\n",
    "            if i + semi == len(sentence):\n",
    "                window.append(sentence[(i+1)])\n",
    "            center.append(word)\n",
    "            context.append(' '.join(window))\n",
    "\n",
    "    with open('wiki.csv', 'w') as f:\n",
    "        writer = csv.writer(f,delimiter='\\t')\n",
    "        writer.writerows(zip(center, context))\n",
    "\n",
    "        \n",
    "corpus_reader(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-e2M8VXh1fh"
   },
   "source": [
    "We sampled 50 000 senteces completely random from the *whole* wikipedia for our training data. Give some reasons why this is good, and why it might be bad. (*note*: We'll have a few questions like these, one or two reasons for and against is sufficient)\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-JEyppsh1fh"
   },
   "source": [
    "--> This can be good because when creating a model, we don't know exactly on what kind of data it will be tested on or needed for. So probably it will be more effective for real-life problems. Typically a larger sample leads to more accurate results and represents a great variety due to significant amount of information. Selecting random samples can also eliminate sampling bias.\n",
    "\n",
    "--> However, a smaller dataset can improve the model training reliability. Also, it needs to be ensured that the sentences  cover many topics when it comes to wikipedia articles, otherwise the model will likely present better results only when used on specific vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ky5zjTFHh1fi"
   },
   "source": [
    "### Loading the data\n",
    "\n",
    "We now need to load the data in an appropriate format for torchtext (https://torchtext.readthedocs.io/en/latest/). We'll use PyText for this and it'll follow the same structure as I showed you in the lecture (remember to lower-case all tokens). Create a function which returns a (bucket)iterator of the training data, and the vocabulary object (```Field```). \n",
    "\n",
    "(*hint1*: you can format the data such that the center word always is first, then you only need to use one field)\n",
    "\n",
    "(*hint2*: the code I showed you during the leture is available in /files/pytorch_tutorial/ on canvas)\n",
    "\n",
    "[4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GP0xt1Clh1fi",
    "outputId": "3dc73019-3598-463a-bca3-0841c43e30ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torchtext.legacy.data.iterator.BucketIterator at 0x7f127aec6850>,\n",
       " <torchtext.legacy.data.field.Field at 0x7f12c3b4c910>)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.legacy.data import Field, BucketIterator, TabularDataset\n",
    "\n",
    "def get_data():\n",
    "    \n",
    "    whitespacer = lambda x: x.split(' ')\n",
    "    \n",
    "    # \"fields\" that process the different columns in our CSV files\n",
    "    ALL_WORDS = Field(tokenize    = whitespacer,\n",
    "                   lower       = True,\n",
    "                   batch_first = True) # enforce the (batch, words) structure\n",
    "\n",
    "\n",
    "    # read the csv files\n",
    "    train = TabularDataset(path = 'wiki.csv',\n",
    "                                  format = 'csv',\n",
    "                                  fields = [('center', ALL_WORDS), #column names and Field\n",
    "                                            ('context',ALL_WORDS)],\n",
    "                                  skip_header= True,\n",
    "                                  csv_reader_params = {'delimiter':'\\t','quotechar':'å'})\n",
    "\n",
    "    # build vocabularies based on what our csv files contained and create word2id mapping\n",
    "    ALL_WORDS.build_vocab(train) #dataset in the parenthesis\n",
    "\n",
    "    # create batches from our data, and shuffle them for each epoch\n",
    "    train_iter = BucketIterator(train,\n",
    "                                                  batch_size        = 8,\n",
    "                                                  sort_within_batch = True,\n",
    "                                                  sort_key          = lambda x: len(x.center),\n",
    "                                                  shuffle           = True,\n",
    "                                                  device            = device)\n",
    "    \n",
    "    return train_iter,ALL_WORDS\n",
    "    \n",
    "\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ODj7Mznh1fk"
   },
   "source": [
    "We lower-cased all tokens above; give some reasons why this is a good idea, and why it may be harmful to our embeddings.\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SP3yzROZh1fk"
   },
   "source": [
    "--> A positive impact would be that most words have the same meaning either written/start with capital letter or not. For instance, the word \"town\" and \"Town\" should have the same vector-semantic representation since it is actually the same word.\n",
    "\n",
    "--> At the same time though, this could be the source of our problem, because some words starting with capital letter have different meaning than when written in lowercase. For example, Mr. \"White\" should have a different semantic representation than \"white\". Generally, it would not treat efficiently name entities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilrH926Nh1fk"
   },
   "source": [
    "## Word Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55hbj0BUh1fl"
   },
   "source": [
    "We will implement the CBOW model for constructing word embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "8RQ78qhfh1fl"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYoip2PVh1fl"
   },
   "source": [
    "In the CBOW model we try to predict the center word based on the context. That is, we take as input ```n``` context words, encode them as vectors, then combine them by summation. This will give us one embedding. We then use this embedding to predict *which* word in our vocabuary is the most likely center word. \n",
    "\n",
    "Implement this model \n",
    "\n",
    "[7 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "bsjidnMAh1fl"
   },
   "outputs": [],
   "source": [
    "# model implementation based on lecture's example\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):# we tried to use the \"embedding_size\" as well as a parameter but it always raised a RuntimeError: CUDA error: device-side assert triggered\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim) # matrix containing the word embeddings\n",
    "        self.prediction = nn.Linear(embedding_dim, vocab_size) # predict which class an example belongs to\n",
    "    # we believe though that self.prediction should be \"nn.Linear(embedding_dim, embedding_size)\n",
    "    \n",
    "    def forward(self, context):\n",
    "        embedded_context = self.embeddings(context)\n",
    "        projection = self.projection_function(embedded_context)\n",
    "        predictions = self.prediction(projection)\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    def projection_function(self, xs):\n",
    "        \"\"\"\n",
    "        This function will take as input a tensor of size (B, S, D)\n",
    "        where B is the batch_size, S the window size, and D the dimensionality of embeddings\n",
    "        this function should compute the sum over the embedding dimensions of the input, \n",
    "        that is, we transform (B, S, D) to (B, 1, D) or (B, D) \n",
    "        \"\"\"\n",
    "\n",
    "        xs_sum = torch.sum(xs, dim=1) \n",
    "        return xs_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yRPYpPwh1fl"
   },
   "source": [
    "Now we need to train the models. First we define which hyperparameters to use. (You can change these, for example when *developing* your model you can use a batch size of 2 and a very low dimensionality (say 10), just to speed things up). When actually training your model *fo real*, you can use a batch size of [8,16,32,64], and embedding dimensionality of [128,256]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "rYdP3Pmyh1fm"
   },
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs :)\n",
    "word_embeddings_hyperparameters = {'epochs':3,\n",
    "                                   'batch_size':16,\n",
    "                                   'embedding_size':128,\n",
    "                                   'learning_rate':0.001,\n",
    "                                   'embedding_dim':128}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FccrJK-dh1fm"
   },
   "source": [
    "Train your model. Iterate over the dataset, get outputs from your model, calculate loss and backpropagate.\n",
    "\n",
    "We mentioned in the lecture that we use Negative Log Likelihood (https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) loss to train Word2Vec model. In this lab we'll take a shortcut when *training* and use Cross Entropy Loss (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), basically it combines ```log_softmax``` and ```NLLLoss```. So what your model should output is a *score* for each word in our vocabulary. The ```CrossEntropyLoss``` will then assign probabilities and calculate the negative log likelihood loss.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "faiITCWlh1fm",
    "outputId": "f10376b4-0ccd-4998-de1a-5b5746ee339c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.298033145521373\n",
      "15.918620811999503\n",
      "23.285724029187485\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataset, vocab = get_data() \n",
    "\n",
    "# build model and construct loss/optimizer --> based on lecture's example\n",
    "cbow_model = CBOWModel(len(vocab.vocab), word_embeddings_hyperparameters['embedding_dim'])\n",
    "cbow_model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=word_embeddings_hyperparameters['learning_rate'])\n",
    "\n",
    "# start training loop\n",
    "cbow_model.train()\n",
    "total_loss = 0\n",
    "for epoch in range(word_embeddings_hyperparameters['epochs']):\n",
    "    for i, batch in enumerate(dataset):\n",
    "        \n",
    "        context = batch.context\n",
    "        target_word = batch.center\n",
    "       \n",
    "        # send your batch of sentences to the model\n",
    "        output = cbow_model(context)\n",
    "\n",
    "\n",
    "        # compute the loss, you'll need to reshape the input\n",
    "        # you can read more about this is the documentation for\n",
    "        # CrossEntropyLoss\n",
    "        loss = loss_fn(output, target_word.view(-1))\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # print average loss for the epoch\n",
    "        print(total_loss/(i+1), end='\\r') \n",
    "        \n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "    print()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeDYUSKXh1fn"
   },
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_hpoljyh1fn"
   },
   "source": [
    "We will evaluate the model on a dataset of word similarities, WordSim353 (http://alfonseca.org/eng/research/wordsim353.html , also avalable in vanvas under files/03-l). The first thing we need to do is read the dataset and translate it to integers. What we'll do is to reuse the ```Field``` that records word indexes (the second output of ```get_data()```) and use it to parse the file.\n",
    "\n",
    "The wordsim data is structured as follows:\n",
    "\n",
    "```\n",
    "word1 word2 score\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "The ```Field``` we got from ```read_data()``` has two built-in functions, ```stoi``` which maps a string to an integer and ```itos``` which maps an integer to a string. \n",
    "\n",
    "What our datareader needs to do is: \n",
    "\n",
    "```\n",
    "for line in file:\n",
    "    word1, word2, score = file.split()\n",
    "    # encode word1 and word2 as integers\n",
    "    word1_idx = vocab.vocab.stoi[word1]\n",
    "    word2_idx = vocab.vocab.stoi[word2]\n",
    "```\n",
    "\n",
    "when we have the integers for ```word_1``` and ```word2``` we'll compute the similarity between their word embeddings with *cosine simlarity*. We can obtain the embeddings by querying the embedding layer of the model.\n",
    "\n",
    "We calculate the cosine similarity for each word pair in the dataset, then compute the pearson correlation between the similarities we obtained with the scores given in the dataset. \n",
    "\n",
    "[4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCJFlaGRh1fo",
    "outputId": "5f9e3fb5-1ee5-40a8-c533-673ae8a9c74c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        0.1741725]\n",
      " [0.1741725 1.       ]]\n",
      "\n",
      "10 best results\n",
      "('tiger , tiger', 0.0)\n",
      "('drink , ear', 0.020552406966686254)\n",
      "('stock , jaguar', 0.03424279001355171)\n",
      "('Wednesday , news', 0.036965128898620636)\n",
      "('possibility , girl', 0.0426633449792862)\n",
      "('professor , cucumber', -0.04335581833124161)\n",
      "('lad , wizard', -0.04467936623096466)\n",
      "('energy , secretary', 0.04526840722560882)\n",
      "('king , cabbage', 0.047713171645998954)\n",
      "('noon , string', 0.05738326510973275)\n",
      "\n",
      "10 worst results\n",
      "('murder , manslaughter', 0.8789119216352701)\n",
      "('boy , lad', 0.8974726941362023)\n",
      "('seafood , food', 0.9067395340800285)\n",
      "('street , avenue', 0.9707365666627885)\n",
      "('mile , kilometer', 0.9790701974034309)\n",
      "('asylum , madhouse', 0.9814636315107345)\n",
      "('fuck , sex', 0.9878224785029888)\n",
      "('money , currency', 0.9935328223705291)\n",
      "('seafood , lobster', 1.0044269663095473)\n",
      "('liquid , water', 1.0166335209608077)\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "cbow_model.eval()\n",
    "pair = {}\n",
    "def read_wordsim(path, vocab, embeddings):\n",
    "    dataset_sims = []\n",
    "    model_sims = []\n",
    "    \n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            word1, word2, score = line.split()\n",
    "\n",
    "            pair[(word1 +\" , \" + word2)] = None\n",
    "            score = float(score)\n",
    "            dataset_sims.append(score)\n",
    "            \n",
    "            # get the index for the word\n",
    "            word1_idx = vocab.vocab.stoi[word1]\n",
    "            word2_idx = vocab.vocab.stoi[word2]\n",
    "            \n",
    "            # get the embedding of the word\n",
    "            word1_emb = embeddings(torch.tensor(word1_idx, device=device).long()) #discord suggestion\n",
    "            word2_emb = embeddings(torch.tensor(word2_idx, device=device).long())\n",
    "            word1_emb = word1_emb.unsqueeze(0)\n",
    "            word2_emb = word2_emb.unsqueeze(0)\n",
    "            # compute cosine similarity, we'll use the version included in pytorch functional\n",
    "            # https://pytorch.org/docs/master/generated/torch.nn.functional.cosine_similarity.html\n",
    "            cosine_similarity = F.cosine_similarity(word1_emb,word2_emb)\n",
    "            \n",
    "            model_sims.append(cosine_similarity.item())\n",
    "            \n",
    "    return dataset_sims, model_sims\n",
    "\n",
    "path = './wordsim_similarity_goldstandard.txt'\n",
    "data, model = read_wordsim(path,vocab,cbow_model.embeddings)\n",
    "pearson_correlation = np.corrcoef(data, model)\n",
    "\n",
    "# the non-diagonals give the pearson correlation,\n",
    "print(pearson_correlation)\n",
    "\n",
    "\n",
    "# compare 10 best and 10 worst corellations\n",
    "all_comparisons = []\n",
    "for i,v in enumerate(data):\n",
    "    data[i] = v/10 # normalise the ground truth data to the model's results (10 is the best result in ground truth set)\n",
    "    all_comparisons.append(data[i]-model[i]) # abstract ground truth-model results\n",
    "    \n",
    "# add the new results as values to the word-pairs keys in the dictionary\n",
    "index = 0\n",
    "for key in pair:\n",
    "    pair[key] = all_comparisons[index]\n",
    "    index+=1\n",
    "\n",
    "# after the abstract, if a value is close to 0, it means that it is a good performing pair\n",
    "pair = sorted(pair.items(), key=lambda x: abs(x[1]))\n",
    "print(\"\")\n",
    "print(\"10 best results\")\n",
    "for i in range(10):\n",
    "    print (pair[i])\n",
    "\n",
    "print(\"\")\n",
    "print(\"10 worst results\")\n",
    "for i in range(-11, -1):\n",
    "    print (pair[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJR_M7wph1fo"
   },
   "source": [
    "Do you think the model performs good or bad? Why?\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQ33-ftVh1fp"
   },
   "source": [
    "--> It seems that the performance is rather weak because the coefficient value (r) is not close to 1, which indicates a non strong relationship between the variables. Specifically, the value +0.6 indicates a moderate positive correlation and the value 0 indicates no correlation, thus our model is somewhere in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4u1EhHRdh1fp"
   },
   "source": [
    "Select the 10 best and 10 worst performing word pairs, can you see any patterns that explain why *these* are the best and worst word pairs?\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ex3ZaXDh1fp"
   },
   "source": [
    "--> The results are printed above.\n",
    "\n",
    "Apart from the word \"tiger\" which is indeed the best result since word1==word2, it seems that there is no relationship between the words included in the successful results. On the other hand, the word pairs included in the worst results are in essence more or less synonyms. We would expect this classification to be the reversed (the 10 worst results are in fact the best results), unless of course the purpose of the model is to extract disimilar words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PELv2MP5h1fp"
   },
   "source": [
    "Suggest some ways of improving the model we apply to WordSim353.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad4Q5xcah1fp"
   },
   "source": [
    "--> The model could be improved if tuning the parameters. By increasing the batch size and the number of epochs, we can enhance the model's performance. The more epochs of course does not necessarily mean better results, so we need to test that number until we notice overfitting in our model.\n",
    "\n",
    "--> Also, the dataset includes 10.000 sentences. We could test that in a different set of 10.000 sentences just to see whether the type of sata affects the model's performance. Otherwise, as the rule of the thumb, by increasing the number of data (more than 10.000 sentences), we can generate a better model, even if training time will increase as well.\n",
    "\n",
    "--> Tha dataset is already rather clean since punctuation marks are removed. We believe that stop words should not be removed since if we want to predict the next word, this could be a stop word, so it has to be maintained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNgY9BGUh1fp"
   },
   "source": [
    "If we consider a scenario where we use these embeddings in a downstream task, for example sentiment analysis (roughly: determining whether a sentence is positive or negative). \n",
    "\n",
    "Give some examples why the sentiment analysis model would benefit from our embeddnings and one examples why our embeddings could hur the performance of the sentiment model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yd8wYX51h1fp"
   },
   "source": [
    "--> The embeddings that we trained have a window size of 4 words which would be significantly useful for a sentiment analysis task, because at times some phrases could be used metaphorically or in an ironic way. Thus, these embeddings could probably assist the model in identifying if that kind of prase is positive or not by the rest of the context.\n",
    "\n",
    "--> If there is one area in which our model seems to fail, it is when attempting to detect synonym words. Thus, if the sentiment analysis task has a corresponding objective, then probably our embeddings would do more harm than good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aPAZnz8h1fq"
   },
   "source": [
    "# Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGH15n0Zh1fq"
   },
   "source": [
    "In this second part we'll build a simple LSTM language model. Your task is to construct a model which takes a sentence as input and predict the next word for each word in the sentence. For this you'll use the ```LSTM``` class provided by PyTorch (https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). You can read more about the LSTM here: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "NOTE!!!: Use the same dataset (wiki-cropus.txt) as before.\n",
    "\n",
    "Our setup is similar to before, we first encode the words as distributed representations then pass these to the LSTM and for each output we predict the next word.\n",
    "\n",
    "For this we'll build a new dataloader with torchtext, the file we pass to the dataloader should contain one sentence per line, with words separated by whitespace.\n",
    "\n",
    "```\n",
    "word_1, ..., word_n\n",
    "word_1, ..., word_k\n",
    "...\n",
    "```\n",
    "\n",
    "in this dataloader you want to make sure that each sentence begins with a ```<start>``` token and ends with a ```<end>``` token, there is a keyword argument in ```Field``` for this :). But other than that, as before you read the dataset and output a iterator over the dataset and a vocabulary. \n",
    "\n",
    "Implement the dataloader, language model and the training loop (the training loop will basically be the same as for word2vec).\n",
    "\n",
    "[12 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "n5uB3j-Vh1fq"
   },
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs as before :)\n",
    "lm_hyperparameters = {'epochs':3,\n",
    "                      'batch_size':16,\n",
    "                      'learning_rate':0.001,\n",
    "                      'embedding_dim':128,\n",
    "                      'output_dim':128}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zrKTnuPsh1fq",
    "outputId": "7f4c5555-48b4-43fc-fb62-6e6e7ab2cc6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torchtext.legacy.data.iterator.BucketIterator at 0x7f126d115850>,\n",
       " <torchtext.legacy.data.field.Field at 0x7f12c3e531d0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_path = './wiki-corpus.txt'\n",
    "def corpus_reader(data_path):\n",
    "    # your code here, roughly the same as for the word2vec dataloader\n",
    "    with open(data_path) as file1:\n",
    "        content = ['sentences']\n",
    "        content.extend(file1.readlines())\n",
    "        with open('wiki2.csv', 'w') as file2:\n",
    "            writer = csv.writer(file2)\n",
    "            writer.writerows([[item.strip()] for item in content])\n",
    "\n",
    "corpus_reader(data_path)\n",
    "\n",
    "\n",
    "\n",
    "from torchtext.legacy.data import Field, BucketIterator, TabularDataset\n",
    "\n",
    "def get_data():\n",
    "    whitespacer = lambda x: x.split(' ')\n",
    "    \n",
    "    # \"fields\" that process the different columns in our CSV files\n",
    "    ALL_WORDS = Field(tokenize    = whitespacer,\n",
    "                   lower       = True,\n",
    "                   batch_first = True, # enforce the (batch, words) structure\n",
    "                   init_token  = '<start>',\n",
    "                   eos_token   = '<end>') \n",
    "\n",
    "\n",
    "    # read the csv files\n",
    "    train = TabularDataset(path = 'wiki2.csv',\n",
    "                                  format = 'csv',\n",
    "                                  fields = [('sentences', ALL_WORDS)],\n",
    "                                  skip_header = True,\n",
    "                                  csv_reader_params = {'delimiter':'\\t','quotechar':'å'})\n",
    "\n",
    "    # build vocabularies based on what our csv files contained and create word2id mapping\n",
    "    #CENTER.build_vocab(train, min_freq=3)\n",
    "    ALL_WORDS.build_vocab(train)\n",
    "\n",
    "    # create batches from our data, and shuffle them for each epoch\n",
    "    train_iter = BucketIterator(train,\n",
    "                                                  batch_size        = 8,\n",
    "                                                  sort_within_batch = True,\n",
    "                                                  sort_key          = lambda x: len(x.sentences),\n",
    "                                                  shuffle           = True,\n",
    "                                                  device            = device)\n",
    "    return train_iter,ALL_WORDS\n",
    "    \n",
    "\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "AOFWed1rh1fq"
   },
   "outputs": [],
   "source": [
    "class LM_withLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "        super(LM_withLSTM, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, output_dim) \n",
    "        self.predict_word = nn.Linear(output_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        embedded_seq = self.embeddings(seq)\n",
    "        timestep_reprentation, *_ = self.LSTM(embedded_seq)\n",
    "        predicted_words = self.predict_word(timestep_reprentation)\n",
    "        \n",
    "        return predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ux_txILlh1fr",
    "outputId": "3bf78070-915b-412e-fbe8-ebabbea36461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.026042938232422\r",
      "8.019185543060303\r",
      "8.016671816507975\r",
      "8.011138439178467\r",
      "8.007717037200928\r",
      "8.003782749176025\r",
      "7.9966495377676825\r",
      "7.9942373633384705\r",
      "7.985994868808323\r",
      "7.980050945281983\r",
      "7.979247786782005\r",
      "7.9784897565841675\r",
      "7.973933623387263\r",
      "7.968300478799002\r",
      "7.957015101114909\r",
      "7.950816303491592\r",
      "7.947763555190143\r",
      "7.94557073381212\r",
      "7.933479309082031\r",
      "7.927370071411133\r",
      "7.92108998979841\r",
      "7.917476350610906\r",
      "7.907068086707073\r",
      "7.901883323987325\r",
      "7.896083965301513\r",
      "7.8887347258054294\r",
      "7.88437196943495\r",
      "7.879316074507577\r",
      "7.857401979380641\r",
      "7.849896860122681\r",
      "7.843294835859729\r",
      "7.8373464196920395\r",
      "7.832094033559163\r",
      "7.827210580601411\r",
      "7.818052782331194\r",
      "7.799246509869893\r",
      "7.790289363345584\r",
      "7.780652673620927\r\n",
      "14.438598883779425\n",
      "20.639627092762996\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataset, vocab = get_data()\n",
    "\n",
    "# build model and construct loss/optimizer\n",
    "lm_model = LM_withLSTM(len(vocab.vocab), \n",
    "                       lm_hyperparameters['embedding_dim'],\n",
    "                       lm_hyperparameters['output_dim'])\n",
    "lm_model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lm_model.parameters(), lr=lm_hyperparameters['learning_rate'])\n",
    "\n",
    "# start training loop\n",
    "lm_model.train()\n",
    "total_loss = 0\n",
    "for epoch in range(lm_hyperparameters['epochs']):\n",
    "    for i, batch in enumerate(dataset):\n",
    "        \n",
    "        # the strucure for each BATCH is:\n",
    "        # <start>, w0, ..., wn, <end>\n",
    "        sentence = batch.sentences\n",
    "\n",
    "        \n",
    "        # when training the model, at each input we predict the *NEXT* token\n",
    "        # consequently there is nothing to predict when we give the model \n",
    "        # <end> as input. \n",
    "        # thus, we do not want to give <end> as input to the model, select \n",
    "        # from each batch all tokens except the last. \n",
    "        # tip: use pytorch indexing/slicing (same as numpy) \n",
    "        # (https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html#operations-on-tensors)\n",
    "        # (https://jhui.github.io/2018/02/09/PyTorch-Basic-operations/)\n",
    "        input_sentence = sentence[:, :-1] \n",
    "       \n",
    "        \n",
    "        \n",
    "        # send your batch of sentences to the model\n",
    "        output = lm_model(input_sentence)\n",
    "        \n",
    "        # for each output, the model predict the NEXT token, so we have to reshape \n",
    "        # our dataset again. On timestep t, we evaluate on token t+1. That is,\n",
    "        # we never predict the <start> token ;) so this time, we select all but the first \n",
    "        # token from sentences (that is, all the tokens that we predict)\n",
    "        gold_data = sentence[:, 1:]\n",
    "        \n",
    "        \n",
    "        # the shape of the output and sentence variable need to be changed,\n",
    "        # for the loss function. Details are in the documentation.\n",
    "        # You can use .view(...,...) to reshape the tensors\n",
    "        \n",
    "        # the output should be of shape (batch_size, seq_len, num_classes)\n",
    "        # we need to consider the size of our vocabulary and the number of classes (the size of the gold_data)\n",
    "        # reshape our output according to the num_classes\n",
    "        loss = loss_fn(output.view(len(gold_data.reshape(-1)), len(vocab.vocab)), gold_data.reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # print average loss for the epoch\n",
    "        print(total_loss/(i+1), end='\\r') \n",
    "        \n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWUrE83Yh1fr"
   },
   "source": [
    "### Evaluating the language model\n",
    "\n",
    "We'll evaluate our model using the BLiMP dataset (https://github.com/alexwarstadt/blimp). The BLiMP dataset contains sets of linguistic minimal pairs for various syntactic and semantic phenomena, We'll evaluate our model on *existential quantifiers* (link: https://github.com/alexwarstadt/blimp/blob/master/data/existential_there_quantifiers_1.jsonl). This data, as the name suggests, investigate whether language models assign higher probability to *correct* usage of there-quantifiers. \n",
    "\n",
    "An example entry in the dataset is: \n",
    "\n",
    "```\n",
    "{\"sentence_good\": \"There was a documentary about music irritating Allison.\", \"sentence_bad\": \"There was each documentary about music irritating Allison.\", \"field\": \"semantics\", \"linguistics_term\": \"quantifiers\", \"UID\": \"existential_there_quantifiers_1\", \"simple_LM_method\": true, \"one_prefix_method\": false, \"two_prefix_method\": false, \"lexically_identical\": false, \"pairID\": \"0\"}\n",
    "```\n",
    "\n",
    "Download the dataset and build a datareader (similar to what you did for word2vec). The dataset structure you should aim for is (you don't need to worry about the other keys for this assignment):\n",
    "\n",
    "```\n",
    "good_sentence_1, bad_sentence_1\n",
    "...\n",
    "```\n",
    "\n",
    "your task now is to compare the probability assigned to the good sentence with to the probability assigned to the bad sentence. To compute a probability for a sentence we consider the product of the probabilities assigned to the *gold* tokens, remember, at timestep ```t``` we're predicting which token comes *next* e.g. ```t+1``` (basically, you do the same thing as you did when training).\n",
    "\n",
    "In rough pseudo code what your code should do is:\n",
    "\n",
    "```\n",
    "accuracy = []\n",
    "for good_sentence, bad_sentence in dataset:\n",
    "    gs_lm_output = LanguageModel(good_sentence)\n",
    "    gs_token_probabilities = softmax(gs_lm_output)\n",
    "    gs_sentence_probability = product(gs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    bs_lm_output = LanguageModel(bad_sentence)\n",
    "    bs_token_probabilities = softmax(bs_lm_output)\n",
    "    bs_sentence_probability = product(bs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    # int(True) = 1 and int(False) = 0\n",
    "    is_correct = int(gs_sentence_probability > bs_sentence_probability)\n",
    "    accuracy.append(is_correct)\n",
    "\n",
    "print(numpy.mean(accuracy))\n",
    "    \n",
    "```\n",
    "\n",
    "[6 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ekhbd-gvh1fr",
    "outputId": "8db34744-9191-4e0e-bb86-cca4ddb42f26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy:\n",
      "0.421\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "import json\n",
    "\n",
    "def evaluate_model(path, vocab, model):\n",
    "    model.eval()\n",
    "    accuracy = []\n",
    "    with open(path) as f:\n",
    "        # iterate over one pair of sentences at a time\n",
    "        for line in f:\n",
    "            # load the data\n",
    "            data = json.loads(line)\n",
    "            good_s = data['sentence_good']\n",
    "            bad_s = data['sentence_bad']\n",
    "            \n",
    "            # the data is tokenized as whitespace\n",
    "            tok_good_s = good_s.split(\" \")\n",
    "            tok_bad_s = bad_s.split(\" \")\n",
    "            \n",
    "            # encode your words as integers using the vocab from the dataloader, size is (S)\n",
    "            # we use unsqueeze to create the batch dimension \n",
    "            # in this case our input is only ONE batch, so the size of the tensor becomes: \n",
    "            # (S) -> (1, S) as the model expects batches\n",
    "            enc_good_s = torch.tensor([vocab.vocab[x] for x in tok_good_s], device=device).unsqueeze(0)\n",
    "            enc_bad_s = torch.tensor([vocab.vocab[x] for x in tok_bad_s], device=device).unsqueeze(0)\n",
    "            \n",
    "            \n",
    "            # pass your encoded sentences to the model and predict the next tokens\n",
    "            good_s = model(enc_good_s)\n",
    "            bad_s = model(enc_bad_s)\n",
    "            \n",
    "            # get probabilities with softmax\n",
    "            # this is usually the last operation done in a network \n",
    "            # A dimension along which softmax will be computed.\n",
    "            f = nn.Softmax(dim=1)\n",
    "            gs_probs = f(good_s)\n",
    "            bs_probs = f(bad_s)\n",
    "            \n",
    "            # select the probability of the gold tokens\n",
    "            gs_sent_prob = find_token_probs(gs_probs, enc_good_s)\n",
    "            bs_sent_prob = find_token_probs(bs_probs, enc_bad_s)\n",
    "\n",
    "            accuracy.append(int(gs_sent_prob>bs_sent_prob))\n",
    "            \n",
    "    return accuracy\n",
    "            \n",
    "def find_token_probs(model_probs, encoded_sentece):\n",
    "    probs = []\n",
    "\n",
    "    # iterate over the tokens in your encoded sentence\n",
    "    for token, gold_token in enumerate(encoded_sentece):\n",
    "        # select the probability of the gold tokens and save\n",
    "        # hint: pytorch indexing is helpful here ;)\n",
    "        prob = model_probs.squeeze(0)[token, gold_token]\n",
    "        probs.append(prob)\n",
    "\n",
    "    # \"prod\" Returns the product of all elements in the input tensor\n",
    "    # \"cat\" concatenates the given sequence of seq tensors \n",
    "    sentence_prob = torch.prod(torch.cat(probs))\n",
    "    return sentence_prob\n",
    "\n",
    "path = \"./existential_there_quantifiers_1.jsonl\"\n",
    "accuracy = evaluate_model(path, vocab, lm_model)\n",
    "\n",
    "print('Final accuracy:')\n",
    "print(np.round(np.mean(accuracy), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7J91b1oh1fs"
   },
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFzQXPM3h1fs"
   },
   "source": [
    "Our model get some score, say, 55% correct predictions. Is this good? Suggest some *baseline* (i.e. a stupid \"model\" we hope ours is better than) we can compare the model against.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Sh7BB75h1fs"
   },
   "source": [
    "Accuracy is only a single metric to draw conclusions concerning our model's effectiveness. Usually, accuracy alone does not gine much information. Let's say though that we trained a model with a baseline less than 55% or around 50%. Also, in order to be able to make a fair comparison, we should consider that we use not only the same dataset, but also the same test set. If the accuracy is too high, maybe this is an overfitting result, but this is not our case. If instead the accuracy is too low, then underfitting is clear. A low accuracy means that our model cannot perform correct predictions, so probably it sees less information in the training process. A 50% accuracy means that half of our data in the test set is predicted correctly, which could clearly be just a matter of luck, so we do not think that even the 55% is actually a good accuracy.\n",
    "\n",
    "Actually, when we first tested the model, we applied no pre-processing at all (no punctuation removed etc.) neither to the training dataset, nor the test set , resulting to the accuracy of 0.421, which is low. What is weird though with our model is that its training is completely unstable. The accuracy changed everytime we re-trained the model, from a range of 0.2 to 0.8, which is wide, so we tried to maintain an average for the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmCQUckjh1fs"
   },
   "source": [
    "Suggest some improvements you could make to your language model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XAfxnJQh1fs"
   },
   "source": [
    "--> Using a GPU, the model was trained really fast, so it would make harm to increase the size of the dataset to more than 50.000 sentences.\n",
    "\n",
    "--> A more qualitative pre-processing would likely improve the performance (mainly removing punctuation). We believe that by removing the stop words, we would decrease our model's performance for this task.\n",
    "\n",
    "--> If the size of the dataset is changed and enhanced, it would be wise to tune the parameters as well: increase the the batch size, test the number of epochs and chose the one that does not bring about overfitting and the embeddings' dimension as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hq8QSLMYh1fs"
   },
   "source": [
    "Suggest some other metrics we can use to evaluate our system\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1HC4pOEh1fs"
   },
   "source": [
    "We can retrieve much more information and details concerning our model's performance if using other metrics in combination with accuracy, such as:\n",
    "\n",
    "--> Precision (positive instances out of the total predicted positive instances)\n",
    "\n",
    "--> Recall (Percentage of positive instances out of the total actual positive instances)\n",
    "\n",
    "--> F1 score (mean of precision and recall)\n",
    "\n",
    "--> Confusion Matrix (for better visualization of the above)\n",
    "\n",
    "--> Log-loss (since this could be a binary-classification problem using probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toY_CBY2h1ft"
   },
   "source": [
    "# Literature\n",
    "\n",
    "\n",
    "Neural architectures:\n",
    "* Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. (Links to an external site.) Journal of Machine Learning Research, 3(6):1137–1155, 2003. (Sections 3 and 4 are less relevant today and hence you can glance through them quickly. Instead, look at the Mikolov papers where they describe training word embeddings with the current neural network architectures.)\n",
    "* T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n",
    "* T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0T25agFh1ft"
   },
   "source": [
    "Total marks: 63"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Draft_Word Embeddings and Language Models.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
